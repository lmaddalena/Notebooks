{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596020871996",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting house price using linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "#import tensorflow.feature_column as fc\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download the Boston housing price dataset\n",
    "(http://lib.stat.cmu.edu/datasets/boston)\n",
    "\n",
    "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
    "prices and the demand for clean air', J. Environ. Economics & Management,\n",
    "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
    "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
    "pages 244-261 of the latter.\n",
    "\n",
    "Variables in order:\n",
    "\n",
    "**CRIM:** per capita crime rate by town  \n",
    "**ZN:** proportion of residential land zoned for lots over 25,000 sq.ft.  \n",
    "**INDUS:** proportion of non-retail business acres per town  \n",
    "**CHAS:** Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)  \n",
    "**NOX:** nitric oxides concentration (parts per 10 million)  \n",
    "**RM:** average number of rooms per dwelling  \n",
    "**AGE:** proportion of owner-occupied units built prior to 1940  \n",
    "**DIS:** weighted distances to five Boston employment centres  \n",
    "**RAD:** index of accessibility to radial highways  \n",
    "**TAX:** full-value property-tax rate per \\$10,000  \n",
    "**PTRATIO:** pupil-teacher ratio by town  \n",
    "**B:** 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town  \n",
    "**LSTAT:** % lower status of the population  \n",
    "**MEDV:** Median value of owner-occupied homes in \\$1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'boston_housing' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-56827242d76b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboston_housing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'boston_housing' is not defined"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = ['CRIM', 'ZN', \n",
    "            'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
    "            'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "x_train_df = pd.DataFrame(x_train, columns = features)\n",
    "x_test_df = pd.DataFrame(x_test, columns = features)\n",
    "y_train_df = pd.DataFrame(y_train, columns = ['MEDV'])\n",
    "y_test_df = pd.DataFrame(y_test, columns = ['MEDV'])\n",
    "\n",
    "x_train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(x_train_df[['CRIM', 'ZN', 'INDUS', 'CHAS']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = x_train_df.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mu = x_train_df.mean()\n",
    "sigma = x_train_df.std()\n",
    "x_train_norm = (x_train_df - mu) / sigma\n",
    "x_train_norm.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize the test dataset\n",
    "I used same mu and sigma computed in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_norm = (x_test_df - mu) / sigma\n",
    "x_test_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    N_HIDDEN = 128\n",
    "    N_FEATURES = 13\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        N_HIDDEN,\n",
    "        name='layer_1', \n",
    "        activation='relu', \n",
    "        input_shape=(N_FEATURES,)))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        N_HIDDEN,\n",
    "        name='layer_2',\n",
    "        activation='relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        1,\n",
    "        name='output_layer',        \n",
    "        activation='linear'\n",
    "    ))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(\n",
    "        learning_rate=0.001,\n",
    "        rho=0.9,\n",
    "        momentum=0.0,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "BATCH_SIZE=128\n",
    "EPOCHS=100\n",
    "\n",
    "history = model.fit(\n",
    "            x_train_norm,\n",
    "            y_train_df,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=1,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[tensorboard_callback]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df['epoch'] = history.epoch\n",
    "history_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "#plt.ylim([0, 100])\n",
    "plt.plot(history_df['epoch'], history_df['loss'], label='Training')\n",
    "plt.plot(history_df['epoch'], history_df['val_loss'], label='Validation', linestyle='dashed')\n",
    "plt.legend(loc='upper center', shadow=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(x_test_norm)\n",
    "\n",
    "a = plt.axis(aspect='equal')\n",
    "plt.scatter(y_test, test_predictions)\n",
    "lims = [0, 50]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, mae, mse = model.evaluate(x_test_norm, y_test, verbose=1)\n",
    "\n",
    "print(\"Testing set Mean Absolute Error: {:5.2f}\".format(mae))\n",
    "print(\"Testing set Loss: {:5.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examining metrics in TensorBoard\n",
    "\n",
    "%tensorboard --logdir logs/scalars"
   ]
  }
 ]
}