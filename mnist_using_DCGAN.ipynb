{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "61e27811106b0eff094ccf4c65f60fdc343a4cd46af11344d5f306dadca4da6f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# DCGAN for MNIST digits\n",
    "\n",
    "Proposed in 2016, Deep Convolutional GANs (DCGAN) have become one of the most popular and successful GAN architecture.\n",
    "\n",
    "(from \"Deep Learning with Tensorflow 2 and Keras\" by Antonio Gulli, Amita Kapoor, Sujit Pal)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### required modules"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self, rows, cols, channels, z = 100):\n",
    "        # Input shape\n",
    "        self.img_rows = rows\n",
    "        self.img_cols = cols\n",
    "        self.channels = channels\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = z\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential(name='generator')\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential(name='discriminator')\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)       \n",
    "\n",
    "    def train(self, epochs, batch_size=256, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/dcgan/dcgan_mnist_%d.png\" % epoch)\n",
    "        plt.close()                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " loss: 0.688082, acc.: 55.08%] [G loss: 0.786230]\n",
      "4650 [D loss: 0.689928, acc.: 55.27%] [G loss: 0.806294]\n",
      "4651 [D loss: 0.682833, acc.: 56.05%] [G loss: 0.834547]\n",
      "4652 [D loss: 0.710844, acc.: 48.24%] [G loss: 0.814997]\n",
      "4653 [D loss: 0.682300, acc.: 53.91%] [G loss: 0.818719]\n",
      "4654 [D loss: 0.700060, acc.: 53.91%] [G loss: 0.790849]\n",
      "4655 [D loss: 0.708146, acc.: 50.00%] [G loss: 0.829228]\n",
      "4656 [D loss: 0.706833, acc.: 51.37%] [G loss: 0.790256]\n",
      "4657 [D loss: 0.692680, acc.: 53.32%] [G loss: 0.836258]\n",
      "4658 [D loss: 0.699171, acc.: 51.76%] [G loss: 0.815503]\n",
      "4659 [D loss: 0.686137, acc.: 56.05%] [G loss: 0.800378]\n",
      "4660 [D loss: 0.711854, acc.: 49.80%] [G loss: 0.835385]\n",
      "4661 [D loss: 0.710204, acc.: 50.59%] [G loss: 0.782499]\n",
      "4662 [D loss: 0.711829, acc.: 51.76%] [G loss: 0.785996]\n",
      "4663 [D loss: 0.695168, acc.: 52.15%] [G loss: 0.828909]\n",
      "4664 [D loss: 0.693421, acc.: 55.66%] [G loss: 0.803813]\n",
      "4665 [D loss: 0.682502, acc.: 56.84%] [G loss: 0.820658]\n",
      "4666 [D loss: 0.706951, acc.: 51.17%] [G loss: 0.792545]\n",
      "4667 [D loss: 0.707194, acc.: 52.54%] [G loss: 0.809026]\n",
      "4668 [D loss: 0.698230, acc.: 54.10%] [G loss: 0.804502]\n",
      "4669 [D loss: 0.695556, acc.: 53.91%] [G loss: 0.802559]\n",
      "4670 [D loss: 0.683125, acc.: 56.64%] [G loss: 0.852730]\n",
      "4671 [D loss: 0.697819, acc.: 55.08%] [G loss: 0.823469]\n",
      "4672 [D loss: 0.701913, acc.: 50.78%] [G loss: 0.825930]\n",
      "4673 [D loss: 0.689599, acc.: 57.03%] [G loss: 0.830658]\n",
      "4674 [D loss: 0.689245, acc.: 58.59%] [G loss: 0.808606]\n",
      "4675 [D loss: 0.705080, acc.: 50.78%] [G loss: 0.823682]\n",
      "4676 [D loss: 0.695772, acc.: 56.64%] [G loss: 0.837981]\n",
      "4677 [D loss: 0.708942, acc.: 50.59%] [G loss: 0.806881]\n",
      "4678 [D loss: 0.680827, acc.: 55.47%] [G loss: 0.818376]\n",
      "4679 [D loss: 0.695552, acc.: 55.08%] [G loss: 0.762525]\n",
      "4680 [D loss: 0.719604, acc.: 47.07%] [G loss: 0.803228]\n",
      "4681 [D loss: 0.718739, acc.: 48.24%] [G loss: 0.788734]\n",
      "4682 [D loss: 0.709873, acc.: 52.15%] [G loss: 0.812306]\n",
      "4683 [D loss: 0.702649, acc.: 51.56%] [G loss: 0.807575]\n",
      "4684 [D loss: 0.705425, acc.: 52.93%] [G loss: 0.829690]\n",
      "4685 [D loss: 0.700894, acc.: 52.73%] [G loss: 0.804236]\n",
      "4686 [D loss: 0.678588, acc.: 58.59%] [G loss: 0.820044]\n",
      "4687 [D loss: 0.692267, acc.: 51.37%] [G loss: 0.800485]\n",
      "4688 [D loss: 0.702122, acc.: 52.54%] [G loss: 0.802428]\n",
      "4689 [D loss: 0.700901, acc.: 51.17%] [G loss: 0.808839]\n",
      "4690 [D loss: 0.714681, acc.: 50.20%] [G loss: 0.818454]\n",
      "4691 [D loss: 0.718009, acc.: 49.02%] [G loss: 0.829526]\n",
      "4692 [D loss: 0.713057, acc.: 51.56%] [G loss: 0.832565]\n",
      "4693 [D loss: 0.696959, acc.: 51.76%] [G loss: 0.797515]\n",
      "4694 [D loss: 0.699560, acc.: 53.12%] [G loss: 0.817940]\n",
      "4695 [D loss: 0.686765, acc.: 57.03%] [G loss: 0.827564]\n",
      "4696 [D loss: 0.698061, acc.: 52.54%] [G loss: 0.818517]\n",
      "4697 [D loss: 0.681723, acc.: 57.23%] [G loss: 0.797627]\n",
      "4698 [D loss: 0.686577, acc.: 56.25%] [G loss: 0.773797]\n",
      "4699 [D loss: 0.702466, acc.: 53.52%] [G loss: 0.786656]\n",
      "4700 [D loss: 0.691764, acc.: 54.88%] [G loss: 0.805053]\n",
      "4701 [D loss: 0.689330, acc.: 54.30%] [G loss: 0.812296]\n",
      "4702 [D loss: 0.698231, acc.: 52.34%] [G loss: 0.805622]\n",
      "4703 [D loss: 0.674125, acc.: 56.64%] [G loss: 0.834040]\n",
      "4704 [D loss: 0.709053, acc.: 51.37%] [G loss: 0.815173]\n",
      "4705 [D loss: 0.696955, acc.: 54.69%] [G loss: 0.837724]\n",
      "4706 [D loss: 0.713373, acc.: 51.37%] [G loss: 0.826744]\n",
      "4707 [D loss: 0.722777, acc.: 47.66%] [G loss: 0.818602]\n",
      "4708 [D loss: 0.676716, acc.: 58.79%] [G loss: 0.846051]\n",
      "4709 [D loss: 0.693738, acc.: 55.08%] [G loss: 0.851192]\n",
      "4710 [D loss: 0.712056, acc.: 49.22%] [G loss: 0.809296]\n",
      "4711 [D loss: 0.677187, acc.: 57.81%] [G loss: 0.826833]\n",
      "4712 [D loss: 0.682315, acc.: 57.81%] [G loss: 0.829545]\n",
      "4713 [D loss: 0.719179, acc.: 51.37%] [G loss: 0.782133]\n",
      "4714 [D loss: 0.696619, acc.: 53.12%] [G loss: 0.817728]\n",
      "4715 [D loss: 0.703629, acc.: 52.93%] [G loss: 0.787045]\n",
      "4716 [D loss: 0.689665, acc.: 53.32%] [G loss: 0.819562]\n",
      "4717 [D loss: 0.701241, acc.: 52.34%] [G loss: 0.782102]\n",
      "4718 [D loss: 0.703370, acc.: 50.39%] [G loss: 0.773186]\n",
      "4719 [D loss: 0.670881, acc.: 58.40%] [G loss: 0.842450]\n",
      "4720 [D loss: 0.705095, acc.: 51.17%] [G loss: 0.791070]\n",
      "4721 [D loss: 0.713207, acc.: 49.80%] [G loss: 0.786499]\n",
      "4722 [D loss: 0.702155, acc.: 52.73%] [G loss: 0.766506]\n",
      "4723 [D loss: 0.704659, acc.: 52.73%] [G loss: 0.797864]\n",
      "4724 [D loss: 0.706444, acc.: 53.52%] [G loss: 0.807715]\n",
      "4725 [D loss: 0.697444, acc.: 52.73%] [G loss: 0.839619]\n",
      "4726 [D loss: 0.713350, acc.: 50.78%] [G loss: 0.797455]\n",
      "4727 [D loss: 0.704749, acc.: 51.56%] [G loss: 0.833273]\n",
      "4728 [D loss: 0.690517, acc.: 56.25%] [G loss: 0.791645]\n",
      "4729 [D loss: 0.716936, acc.: 52.15%] [G loss: 0.773034]\n",
      "4730 [D loss: 0.712856, acc.: 51.56%] [G loss: 0.811301]\n",
      "4731 [D loss: 0.690977, acc.: 55.66%] [G loss: 0.812679]\n",
      "4732 [D loss: 0.702784, acc.: 54.49%] [G loss: 0.810887]\n",
      "4733 [D loss: 0.687303, acc.: 52.73%] [G loss: 0.823761]\n",
      "4734 [D loss: 0.712776, acc.: 51.56%] [G loss: 0.834585]\n",
      "4735 [D loss: 0.707212, acc.: 50.00%] [G loss: 0.822241]\n",
      "4736 [D loss: 0.705800, acc.: 51.17%] [G loss: 0.819383]\n",
      "4737 [D loss: 0.704094, acc.: 50.00%] [G loss: 0.827365]\n",
      "4738 [D loss: 0.703197, acc.: 52.34%] [G loss: 0.808548]\n",
      "4739 [D loss: 0.686000, acc.: 53.91%] [G loss: 0.804455]\n",
      "4740 [D loss: 0.686528, acc.: 57.81%] [G loss: 0.823498]\n",
      "4741 [D loss: 0.719973, acc.: 48.24%] [G loss: 0.801153]\n",
      "4742 [D loss: 0.706019, acc.: 54.88%] [G loss: 0.796151]\n",
      "4743 [D loss: 0.708791, acc.: 51.76%] [G loss: 0.795028]\n",
      "4744 [D loss: 0.695866, acc.: 54.88%] [G loss: 0.809544]\n",
      "4745 [D loss: 0.699634, acc.: 51.17%] [G loss: 0.806964]\n",
      "4746 [D loss: 0.697355, acc.: 55.27%] [G loss: 0.841949]\n",
      "4747 [D loss: 0.696092, acc.: 52.54%] [G loss: 0.827401]\n",
      "4748 [D loss: 0.712353, acc.: 49.80%] [G loss: 0.814090]\n",
      "4749 [D loss: 0.717055, acc.: 52.34%] [G loss: 0.789813]\n",
      "4750 [D loss: 0.690299, acc.: 55.66%] [G loss: 0.825720]\n",
      "4751 [D loss: 0.712666, acc.: 51.37%] [G loss: 0.792490]\n",
      "4752 [D loss: 0.688866, acc.: 54.30%] [G loss: 0.842253]\n",
      "4753 [D loss: 0.700131, acc.: 50.78%] [G loss: 0.819116]\n",
      "4754 [D loss: 0.714545, acc.: 49.02%] [G loss: 0.800412]\n",
      "4755 [D loss: 0.681947, acc.: 54.88%] [G loss: 0.834384]\n",
      "4756 [D loss: 0.704561, acc.: 50.59%] [G loss: 0.820980]\n",
      "4757 [D loss: 0.702838, acc.: 55.08%] [G loss: 0.824641]\n",
      "4758 [D loss: 0.697836, acc.: 54.69%] [G loss: 0.800424]\n",
      "4759 [D loss: 0.698456, acc.: 56.25%] [G loss: 0.812671]\n",
      "4760 [D loss: 0.684344, acc.: 53.71%] [G loss: 0.813063]\n",
      "4761 [D loss: 0.705145, acc.: 52.54%] [G loss: 0.804030]\n",
      "4762 [D loss: 0.704590, acc.: 51.76%] [G loss: 0.812548]\n",
      "4763 [D loss: 0.681458, acc.: 58.79%] [G loss: 0.788859]\n",
      "4764 [D loss: 0.705328, acc.: 50.78%] [G loss: 0.795595]\n",
      "4765 [D loss: 0.710401, acc.: 49.41%] [G loss: 0.773287]\n",
      "4766 [D loss: 0.684062, acc.: 57.03%] [G loss: 0.797251]\n",
      "4767 [D loss: 0.685438, acc.: 56.25%] [G loss: 0.828421]\n",
      "4768 [D loss: 0.718395, acc.: 50.39%] [G loss: 0.822947]\n",
      "4769 [D loss: 0.693117, acc.: 53.91%] [G loss: 0.815080]\n",
      "4770 [D loss: 0.709666, acc.: 53.12%] [G loss: 0.841255]\n",
      "4771 [D loss: 0.681117, acc.: 55.86%] [G loss: 0.800057]\n",
      "4772 [D loss: 0.701139, acc.: 55.08%] [G loss: 0.830128]\n",
      "4773 [D loss: 0.686055, acc.: 55.47%] [G loss: 0.817022]\n",
      "4774 [D loss: 0.699384, acc.: 50.78%] [G loss: 0.807221]\n",
      "4775 [D loss: 0.693010, acc.: 56.84%] [G loss: 0.815813]\n",
      "4776 [D loss: 0.703223, acc.: 53.52%] [G loss: 0.818004]\n",
      "4777 [D loss: 0.687685, acc.: 57.03%] [G loss: 0.812067]\n",
      "4778 [D loss: 0.712121, acc.: 51.17%] [G loss: 0.796583]\n",
      "4779 [D loss: 0.687943, acc.: 55.27%] [G loss: 0.805565]\n",
      "4780 [D loss: 0.701659, acc.: 54.30%] [G loss: 0.797049]\n",
      "4781 [D loss: 0.687409, acc.: 55.86%] [G loss: 0.816767]\n",
      "4782 [D loss: 0.682591, acc.: 58.59%] [G loss: 0.787984]\n",
      "4783 [D loss: 0.714459, acc.: 50.78%] [G loss: 0.795930]\n",
      "4784 [D loss: 0.708739, acc.: 52.73%] [G loss: 0.818585]\n",
      "4785 [D loss: 0.680282, acc.: 55.66%] [G loss: 0.841100]\n",
      "4786 [D loss: 0.719140, acc.: 51.37%] [G loss: 0.787017]\n",
      "4787 [D loss: 0.700518, acc.: 50.00%] [G loss: 0.776643]\n",
      "4788 [D loss: 0.693048, acc.: 53.91%] [G loss: 0.835577]\n",
      "4789 [D loss: 0.709065, acc.: 50.59%] [G loss: 0.788466]\n",
      "4790 [D loss: 0.679014, acc.: 57.42%] [G loss: 0.825524]\n",
      "4791 [D loss: 0.709953, acc.: 49.41%] [G loss: 0.798122]\n",
      "4792 [D loss: 0.689271, acc.: 57.62%] [G loss: 0.825422]\n",
      "4793 [D loss: 0.712892, acc.: 49.22%] [G loss: 0.800324]\n",
      "4794 [D loss: 0.690344, acc.: 54.69%] [G loss: 0.776320]\n",
      "4795 [D loss: 0.717658, acc.: 50.39%] [G loss: 0.780710]\n",
      "4796 [D loss: 0.707145, acc.: 50.39%] [G loss: 0.819499]\n",
      "4797 [D loss: 0.691065, acc.: 53.71%] [G loss: 0.818242]\n",
      "4798 [D loss: 0.697008, acc.: 52.54%] [G loss: 0.800466]\n",
      "4799 [D loss: 0.682593, acc.: 57.62%] [G loss: 0.785703]\n",
      "4800 [D loss: 0.703391, acc.: 51.56%] [G loss: 0.793669]\n",
      "4801 [D loss: 0.685670, acc.: 58.20%] [G loss: 0.831785]\n",
      "4802 [D loss: 0.700556, acc.: 51.95%] [G loss: 0.796505]\n",
      "4803 [D loss: 0.696726, acc.: 52.54%] [G loss: 0.819637]\n",
      "4804 [D loss: 0.709933, acc.: 53.71%] [G loss: 0.793814]\n",
      "4805 [D loss: 0.699263, acc.: 54.69%] [G loss: 0.843309]\n",
      "4806 [D loss: 0.704554, acc.: 49.61%] [G loss: 0.780834]\n",
      "4807 [D loss: 0.721233, acc.: 49.22%] [G loss: 0.803404]\n",
      "4808 [D loss: 0.715627, acc.: 50.59%] [G loss: 0.755657]\n",
      "4809 [D loss: 0.679924, acc.: 56.05%] [G loss: 0.789654]\n",
      "4810 [D loss: 0.695438, acc.: 53.91%] [G loss: 0.828863]\n",
      "4811 [D loss: 0.688131, acc.: 54.10%] [G loss: 0.817042]\n",
      "4812 [D loss: 0.698357, acc.: 50.78%] [G loss: 0.812592]\n",
      "4813 [D loss: 0.688170, acc.: 53.12%] [G loss: 0.796976]\n",
      "4814 [D loss: 0.717183, acc.: 46.88%] [G loss: 0.805365]\n",
      "4815 [D loss: 0.701641, acc.: 55.47%] [G loss: 0.825770]\n",
      "4816 [D loss: 0.702057, acc.: 53.71%] [G loss: 0.790762]\n",
      "4817 [D loss: 0.683278, acc.: 55.27%] [G loss: 0.811731]\n",
      "4818 [D loss: 0.734933, acc.: 46.09%] [G loss: 0.792765]\n",
      "4819 [D loss: 0.695593, acc.: 52.15%] [G loss: 0.794228]\n",
      "4820 [D loss: 0.684955, acc.: 55.47%] [G loss: 0.821374]\n",
      "4821 [D loss: 0.697241, acc.: 51.17%] [G loss: 0.809680]\n",
      "4822 [D loss: 0.706773, acc.: 48.24%] [G loss: 0.785677]\n",
      "4823 [D loss: 0.706492, acc.: 50.98%] [G loss: 0.766419]\n",
      "4824 [D loss: 0.697213, acc.: 52.54%] [G loss: 0.803817]\n",
      "4825 [D loss: 0.710109, acc.: 50.39%] [G loss: 0.804067]\n",
      "4826 [D loss: 0.713466, acc.: 50.00%] [G loss: 0.800494]\n",
      "4827 [D loss: 0.717253, acc.: 48.44%] [G loss: 0.800528]\n",
      "4828 [D loss: 0.713110, acc.: 49.61%] [G loss: 0.820211]\n",
      "4829 [D loss: 0.701794, acc.: 53.12%] [G loss: 0.799117]\n",
      "4830 [D loss: 0.708760, acc.: 50.39%] [G loss: 0.788175]\n",
      "4831 [D loss: 0.697545, acc.: 52.73%] [G loss: 0.807870]\n",
      "4832 [D loss: 0.705487, acc.: 50.98%] [G loss: 0.817010]\n",
      "4833 [D loss: 0.698832, acc.: 52.73%] [G loss: 0.839764]\n",
      "4834 [D loss: 0.692216, acc.: 52.54%] [G loss: 0.823143]\n",
      "4835 [D loss: 0.704122, acc.: 53.52%] [G loss: 0.787083]\n",
      "4836 [D loss: 0.695734, acc.: 51.95%] [G loss: 0.854629]\n",
      "4837 [D loss: 0.713950, acc.: 50.78%] [G loss: 0.787273]\n",
      "4838 [D loss: 0.692955, acc.: 51.76%] [G loss: 0.789390]\n",
      "4839 [D loss: 0.702219, acc.: 54.10%] [G loss: 0.815740]\n",
      "4840 [D loss: 0.675908, acc.: 57.23%] [G loss: 0.814312]\n",
      "4841 [D loss: 0.693953, acc.: 56.25%] [G loss: 0.804018]\n",
      "4842 [D loss: 0.712239, acc.: 50.78%] [G loss: 0.769874]\n",
      "4843 [D loss: 0.702053, acc.: 52.93%] [G loss: 0.811697]\n",
      "4844 [D loss: 0.671951, acc.: 58.40%] [G loss: 0.793535]\n",
      "4845 [D loss: 0.695760, acc.: 52.73%] [G loss: 0.847613]\n",
      "4846 [D loss: 0.692404, acc.: 54.10%] [G loss: 0.832616]\n",
      "4847 [D loss: 0.697205, acc.: 53.71%] [G loss: 0.822489]\n",
      "4848 [D loss: 0.693647, acc.: 52.15%] [G loss: 0.801606]\n",
      "4849 [D loss: 0.695896, acc.: 52.54%] [G loss: 0.840921]\n",
      "4850 [D loss: 0.693813, acc.: 53.91%] [G loss: 0.818506]\n",
      "4851 [D loss: 0.696922, acc.: 53.32%] [G loss: 0.804583]\n",
      "4852 [D loss: 0.703687, acc.: 51.56%] [G loss: 0.805302]\n",
      "4853 [D loss: 0.704616, acc.: 52.73%] [G loss: 0.784382]\n",
      "4854 [D loss: 0.679097, acc.: 58.20%] [G loss: 0.819034]\n",
      "4855 [D loss: 0.706808, acc.: 51.17%] [G loss: 0.798385]\n",
      "4856 [D loss: 0.694908, acc.: 53.32%] [G loss: 0.827169]\n",
      "4857 [D loss: 0.712593, acc.: 49.61%] [G loss: 0.791169]\n",
      "4858 [D loss: 0.695294, acc.: 54.49%] [G loss: 0.806940]\n",
      "4859 [D loss: 0.703532, acc.: 52.54%] [G loss: 0.789681]\n",
      "4860 [D loss: 0.702570, acc.: 53.12%] [G loss: 0.805339]\n",
      "4861 [D loss: 0.707719, acc.: 52.93%] [G loss: 0.792133]\n",
      "4862 [D loss: 0.683403, acc.: 55.66%] [G loss: 0.774603]\n",
      "4863 [D loss: 0.718402, acc.: 49.80%] [G loss: 0.810852]\n",
      "4864 [D loss: 0.697619, acc.: 51.76%] [G loss: 0.822228]\n",
      "4865 [D loss: 0.710557, acc.: 52.93%] [G loss: 0.778273]\n",
      "4866 [D loss: 0.704132, acc.: 50.59%] [G loss: 0.790009]\n",
      "4867 [D loss: 0.703282, acc.: 50.39%] [G loss: 0.796524]\n",
      "4868 [D loss: 0.712313, acc.: 48.05%] [G loss: 0.785217]\n",
      "4869 [D loss: 0.689441, acc.: 54.69%] [G loss: 0.808787]\n",
      "4870 [D loss: 0.702244, acc.: 52.73%] [G loss: 0.817832]\n",
      "4871 [D loss: 0.700540, acc.: 50.59%] [G loss: 0.810569]\n",
      "4872 [D loss: 0.693400, acc.: 55.86%] [G loss: 0.808872]\n",
      "4873 [D loss: 0.696212, acc.: 52.73%] [G loss: 0.792664]\n",
      "4874 [D loss: 0.695581, acc.: 55.86%] [G loss: 0.826829]\n",
      "4875 [D loss: 0.691463, acc.: 53.91%] [G loss: 0.817910]\n",
      "4876 [D loss: 0.700816, acc.: 53.91%] [G loss: 0.822562]\n",
      "4877 [D loss: 0.698033, acc.: 54.49%] [G loss: 0.823353]\n",
      "4878 [D loss: 0.707340, acc.: 48.83%] [G loss: 0.799752]\n",
      "4879 [D loss: 0.703474, acc.: 51.95%] [G loss: 0.789922]\n",
      "4880 [D loss: 0.697111, acc.: 53.32%] [G loss: 0.800459]\n",
      "4881 [D loss: 0.703313, acc.: 52.73%] [G loss: 0.789217]\n",
      "4882 [D loss: 0.695087, acc.: 54.49%] [G loss: 0.781809]\n",
      "4883 [D loss: 0.709482, acc.: 51.95%] [G loss: 0.796135]\n",
      "4884 [D loss: 0.711192, acc.: 51.37%] [G loss: 0.777097]\n",
      "4885 [D loss: 0.689555, acc.: 53.71%] [G loss: 0.790991]\n",
      "4886 [D loss: 0.703725, acc.: 52.34%] [G loss: 0.809066]\n",
      "4887 [D loss: 0.693269, acc.: 54.69%] [G loss: 0.806188]\n",
      "4888 [D loss: 0.683914, acc.: 55.47%] [G loss: 0.824152]\n",
      "4889 [D loss: 0.702223, acc.: 52.73%] [G loss: 0.806135]\n",
      "4890 [D loss: 0.695690, acc.: 56.64%] [G loss: 0.789330]\n",
      "4891 [D loss: 0.707864, acc.: 54.88%] [G loss: 0.806750]\n",
      "4892 [D loss: 0.689905, acc.: 53.71%] [G loss: 0.813623]\n",
      "4893 [D loss: 0.715314, acc.: 49.41%] [G loss: 0.774539]\n",
      "4894 [D loss: 0.693614, acc.: 54.10%] [G loss: 0.789241]\n",
      "4895 [D loss: 0.691010, acc.: 55.27%] [G loss: 0.827209]\n",
      "4896 [D loss: 0.690938, acc.: 55.47%] [G loss: 0.789768]\n",
      "4897 [D loss: 0.686835, acc.: 54.49%] [G loss: 0.801122]\n",
      "4898 [D loss: 0.703057, acc.: 51.95%] [G loss: 0.790673]\n",
      "4899 [D loss: 0.691314, acc.: 54.69%] [G loss: 0.802024]\n",
      "4900 [D loss: 0.690772, acc.: 52.73%] [G loss: 0.834172]\n",
      "4901 [D loss: 0.705011, acc.: 52.34%] [G loss: 0.838216]\n",
      "4902 [D loss: 0.666020, acc.: 59.57%] [G loss: 0.823435]\n",
      "4903 [D loss: 0.696616, acc.: 56.64%] [G loss: 0.835341]\n",
      "4904 [D loss: 0.723293, acc.: 47.27%] [G loss: 0.781295]\n",
      "4905 [D loss: 0.714451, acc.: 48.83%] [G loss: 0.788537]\n",
      "4906 [D loss: 0.705988, acc.: 53.12%] [G loss: 0.799507]\n",
      "4907 [D loss: 0.683348, acc.: 56.84%] [G loss: 0.827534]\n",
      "4908 [D loss: 0.700683, acc.: 54.49%] [G loss: 0.816103]\n",
      "4909 [D loss: 0.703921, acc.: 53.91%] [G loss: 0.830413]\n",
      "4910 [D loss: 0.706277, acc.: 51.56%] [G loss: 0.790316]\n",
      "4911 [D loss: 0.700977, acc.: 55.08%] [G loss: 0.805704]\n",
      "4912 [D loss: 0.697719, acc.: 54.49%] [G loss: 0.795646]\n",
      "4913 [D loss: 0.715110, acc.: 50.00%] [G loss: 0.809698]\n",
      "4914 [D loss: 0.704499, acc.: 51.17%] [G loss: 0.822673]\n",
      "4915 [D loss: 0.691339, acc.: 55.08%] [G loss: 0.814716]\n",
      "4916 [D loss: 0.694515, acc.: 51.56%] [G loss: 0.806077]\n",
      "4917 [D loss: 0.691837, acc.: 56.05%] [G loss: 0.828219]\n",
      "4918 [D loss: 0.694688, acc.: 50.39%] [G loss: 0.802789]\n",
      "4919 [D loss: 0.698587, acc.: 51.76%] [G loss: 0.840325]\n",
      "4920 [D loss: 0.706893, acc.: 49.61%] [G loss: 0.839409]\n",
      "4921 [D loss: 0.686482, acc.: 55.08%] [G loss: 0.840564]\n",
      "4922 [D loss: 0.716606, acc.: 50.00%] [G loss: 0.771695]\n",
      "4923 [D loss: 0.710356, acc.: 51.76%] [G loss: 0.832966]\n",
      "4924 [D loss: 0.715943, acc.: 48.05%] [G loss: 0.798587]\n",
      "4925 [D loss: 0.695076, acc.: 53.12%] [G loss: 0.827236]\n",
      "4926 [D loss: 0.690985, acc.: 56.05%] [G loss: 0.835121]\n",
      "4927 [D loss: 0.697040, acc.: 53.12%] [G loss: 0.831116]\n",
      "4928 [D loss: 0.681218, acc.: 56.45%] [G loss: 0.781614]\n",
      "4929 [D loss: 0.703262, acc.: 54.49%] [G loss: 0.797710]\n",
      "4930 [D loss: 0.700229, acc.: 54.69%] [G loss: 0.799329]\n",
      "4931 [D loss: 0.693661, acc.: 54.88%] [G loss: 0.790079]\n",
      "4932 [D loss: 0.701269, acc.: 55.08%] [G loss: 0.788188]\n",
      "4933 [D loss: 0.693326, acc.: 56.05%] [G loss: 0.791586]\n",
      "4934 [D loss: 0.706422, acc.: 50.98%] [G loss: 0.815670]\n",
      "4935 [D loss: 0.708637, acc.: 49.61%] [G loss: 0.826410]\n",
      "4936 [D loss: 0.677924, acc.: 56.25%] [G loss: 0.807847]\n",
      "4937 [D loss: 0.709150, acc.: 50.20%] [G loss: 0.821125]\n",
      "4938 [D loss: 0.685113, acc.: 52.15%] [G loss: 0.781179]\n",
      "4939 [D loss: 0.678421, acc.: 58.59%] [G loss: 0.811264]\n",
      "4940 [D loss: 0.684010, acc.: 54.49%] [G loss: 0.785662]\n",
      "4941 [D loss: 0.690382, acc.: 56.05%] [G loss: 0.782444]\n",
      "4942 [D loss: 0.695229, acc.: 50.78%] [G loss: 0.793267]\n",
      "4943 [D loss: 0.683840, acc.: 57.23%] [G loss: 0.794370]\n",
      "4944 [D loss: 0.705657, acc.: 50.98%] [G loss: 0.817577]\n",
      "4945 [D loss: 0.688257, acc.: 56.05%] [G loss: 0.831370]\n",
      "4946 [D loss: 0.705208, acc.: 51.95%] [G loss: 0.840892]\n",
      "4947 [D loss: 0.672879, acc.: 57.03%] [G loss: 0.833599]\n",
      "4948 [D loss: 0.709146, acc.: 50.98%] [G loss: 0.789145]\n",
      "4949 [D loss: 0.687950, acc.: 55.47%] [G loss: 0.826706]\n",
      "4950 [D loss: 0.681844, acc.: 56.25%] [G loss: 0.802826]\n",
      "4951 [D loss: 0.703734, acc.: 50.98%] [G loss: 0.833861]\n",
      "4952 [D loss: 0.689764, acc.: 55.27%] [G loss: 0.794016]\n",
      "4953 [D loss: 0.697623, acc.: 56.05%] [G loss: 0.802811]\n",
      "4954 [D loss: 0.686397, acc.: 53.71%] [G loss: 0.810609]\n",
      "4955 [D loss: 0.676906, acc.: 57.23%] [G loss: 0.841691]\n",
      "4956 [D loss: 0.717778, acc.: 50.20%] [G loss: 0.814517]\n",
      "4957 [D loss: 0.695357, acc.: 52.34%] [G loss: 0.806784]\n",
      "4958 [D loss: 0.699326, acc.: 53.32%] [G loss: 0.822684]\n",
      "4959 [D loss: 0.705126, acc.: 52.15%] [G loss: 0.831547]\n",
      "4960 [D loss: 0.704322, acc.: 51.56%] [G loss: 0.809444]\n",
      "4961 [D loss: 0.696744, acc.: 53.91%] [G loss: 0.792031]\n",
      "4962 [D loss: 0.700965, acc.: 50.98%] [G loss: 0.778402]\n",
      "4963 [D loss: 0.705182, acc.: 49.02%] [G loss: 0.791015]\n",
      "4964 [D loss: 0.725756, acc.: 47.27%] [G loss: 0.787082]\n",
      "4965 [D loss: 0.704996, acc.: 50.98%] [G loss: 0.822558]\n",
      "4966 [D loss: 0.705213, acc.: 51.37%] [G loss: 0.842097]\n",
      "4967 [D loss: 0.694085, acc.: 53.52%] [G loss: 0.829368]\n",
      "4968 [D loss: 0.696654, acc.: 53.32%] [G loss: 0.806267]\n",
      "4969 [D loss: 0.710176, acc.: 49.22%] [G loss: 0.787076]\n",
      "4970 [D loss: 0.689033, acc.: 53.32%] [G loss: 0.816308]\n",
      "4971 [D loss: 0.692731, acc.: 55.66%] [G loss: 0.827333]\n",
      "4972 [D loss: 0.695395, acc.: 53.91%] [G loss: 0.830567]\n",
      "4973 [D loss: 0.710725, acc.: 51.56%] [G loss: 0.800261]\n",
      "4974 [D loss: 0.680924, acc.: 54.10%] [G loss: 0.813198]\n",
      "4975 [D loss: 0.699576, acc.: 52.73%] [G loss: 0.834511]\n",
      "4976 [D loss: 0.687138, acc.: 55.66%] [G loss: 0.815983]\n",
      "4977 [D loss: 0.678537, acc.: 58.40%] [G loss: 0.824344]\n",
      "4978 [D loss: 0.675076, acc.: 56.45%] [G loss: 0.818320]\n",
      "4979 [D loss: 0.697401, acc.: 54.49%] [G loss: 0.835370]\n",
      "4980 [D loss: 0.712915, acc.: 51.37%] [G loss: 0.780592]\n",
      "4981 [D loss: 0.691449, acc.: 53.71%] [G loss: 0.793685]\n",
      "4982 [D loss: 0.712592, acc.: 54.30%] [G loss: 0.808037]\n",
      "4983 [D loss: 0.698719, acc.: 55.08%] [G loss: 0.778334]\n",
      "4984 [D loss: 0.691998, acc.: 53.91%] [G loss: 0.807834]\n",
      "4985 [D loss: 0.702616, acc.: 53.52%] [G loss: 0.842044]\n",
      "4986 [D loss: 0.705779, acc.: 53.32%] [G loss: 0.791656]\n",
      "4987 [D loss: 0.711704, acc.: 51.37%] [G loss: 0.814734]\n",
      "4988 [D loss: 0.704807, acc.: 51.37%] [G loss: 0.829850]\n",
      "4989 [D loss: 0.707139, acc.: 52.15%] [G loss: 0.814683]\n",
      "4990 [D loss: 0.673093, acc.: 58.01%] [G loss: 0.829472]\n",
      "4991 [D loss: 0.732155, acc.: 46.48%] [G loss: 0.793836]\n",
      "4992 [D loss: 0.695679, acc.: 52.73%] [G loss: 0.815312]\n",
      "4993 [D loss: 0.667861, acc.: 58.98%] [G loss: 0.821075]\n",
      "4994 [D loss: 0.692176, acc.: 57.42%] [G loss: 0.818993]\n",
      "4995 [D loss: 0.700530, acc.: 53.52%] [G loss: 0.827480]\n",
      "4996 [D loss: 0.691900, acc.: 55.08%] [G loss: 0.830844]\n",
      "4997 [D loss: 0.689184, acc.: 55.66%] [G loss: 0.789556]\n",
      "4998 [D loss: 0.700723, acc.: 55.08%] [G loss: 0.807544]\n",
      "4999 [D loss: 0.701371, acc.: 53.32%] [G loss: 0.814835]\n"
     ]
    }
   ],
   "source": [
    "dcgan = DCGAN(28,28,1)\n",
    "\n",
    "dcgan.train(epochs=5000, batch_size=256, save_interval=500)"
   ]
  }
 ]
}